<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en" >
<head>
	<!-- 2026-02-23 Mon 13:53 -->
	<meta charset="UTF-8" />
	<meta name="description" content="" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<meta name="theme-color" content="#4e6b1a" />
		<meta name="theme-color" content="#c3e87f" media="(prefers-color-scheme:dark)" />
	<title>Ollama, LM Studio vs llama.cpp - Mart Roosmaa</title>
	<link rel="canonical" href="https://www.roosmaa.net/blog/2025/ollama-lmstudio-llamacpp/" /><link rel="icon" type="image/png" href="https://www.roosmaa.net/favicon.png" />

<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://www.roosmaa.net/apple-touch-icon.png" />
	<link rel="icon" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'%3E%3Ctext x='-.07em' y='.89em' font-size='90'%3EðŸ“—%3C/text%3E%3C/svg%3E">

		
			
				<link rel="alternate" type="application/rss+xml" title="Mart Roosmaa - RSS Feed" href="https://www.roosmaa.net/rss.xml">
			
		
			
				<link rel="alternate" type="application/atom+xml" title="Mart Roosmaa - Atom Feed" href="https://www.roosmaa.net/atom.xml">
			
		
    <style type="text/css">
	:root {--accent-color: #4e6b1a;}[data-theme="dark"] {
			--accent-color: #c3e87f;
		}

		@media (prefers-color-scheme: dark) {
			:root:not([data-theme="light"]) {
				--accent-color: #c3e87f;
			}
		}</style>

			<link type="text/css" rel="stylesheet" href="https://www.roosmaa.net/style.css" />
			<link type="text/css" rel="stylesheet" href="https://www.roosmaa.net/duckquill_mods.css" />
			<link type="text/css" rel="stylesheet" href="https://www.roosmaa.net/image_gallery.css" />
			<script type="text/javascript" defer  src="https://www.roosmaa.net/closable.js"></script>
			<script type="text/javascript" defer  src="https://www.roosmaa.net/copy-button.js"></script>
			<script type="text/javascript" defer  src="https://www.roosmaa.net/theme-switcher.js"></script>

	<meta property="og:site_name" content="Mart Roosmaa" />
	<meta property="og:title" content="Ollama, LM Studio vs llama.cpp - Mart Roosmaa" />
	<meta property="og:url" content="https://www.roosmaa.net/blog/2025/ollama-lmstudio-llamacpp/" />
	<meta property="og:description" content="Quick comparison of Ollama, LM Studio and llama.cpp" /><meta property="og:image" content="https://www.roosmaa.net/card.png" /><meta property="og:locale" content="en_US" />
</head>

<body>

		
<header id="site-nav">
	<nav>
		<a href="#main-content" tabindex="0">
			Skip to Main Content
		</a>
		<ul>
			<li id="home">
				<a href="https://www.roosmaa.net">
					<i class="icon"></i>Mart Roosmaa</a>
			</li>
			<li class="divider"></li>
					<li>
						<a href="https://www.roosmaa.net/blog/">Blog</a>
					</li>
				<li id="theme-switcher">
					<details class="closable">
						<summary class="circle" title="Theme">
							<i class="icon"></i>
						</summary>
						<ul>
							<li>
								<button class="circle" id="theme-light" title="Switch to Light Theme">
									<i class="icon"></i>
								</button>
							</li>
							<li>
								<button class="circle" id="theme-dark" title="Switch to Dark Theme">
									<i class="icon"></i>
								</button>
							</li>
							<li>
								<button class="circle" id="theme-system" title="Use System Theme">
									<i class="icon"></i>
								</button>
							</li>
						</ul>
					</details>
				</li><li id="feed">
					<details class="closable">
						<summary class="circle" title="Feed">
							<i class="icon"></i>
						</summary>
						<ul>
								<li>
									<a href="https://www.roosmaa.net/rss.xml" rel="">RSS</a>
								</li>
								<li>
									<a href="https://www.roosmaa.net/atom.xml" rel="">Atom</a>
								</li></ul>
					</details>
				</li>
		</ul>
	</nav>
</header>
<main id="main-content">
		<article><div id="heading"><p>
				<small>
					<time datetime=" 2025-12-24T00:00:00+00:00">Published on
						December 24, 2025</time></small>
			</p><h1>Ollama, LM Studio vs llama.cpp</h1><p>
				<small><span>5 minutes read</span><span> â€¢ </span></small>
			</p>
				<ul class="tags"><li><a class="tag" href="https://www.roosmaa.net/tags/local-ai/">local ai</a></li><li><a class="tag" href="https://www.roosmaa.net/tags/ollama/">ollama</a></li><li><a class="tag" href="https://www.roosmaa.net/tags/lmstudio/">lmstudio</a></li><li><a class="tag" href="https://www.roosmaa.net/tags/llama-cpp/">llama.cpp</a></li>
				</ul>
	</div>

	<div id="buttons-container"><a id="go-to-top" href="#top" title="Go to Top"><i class="icon"></i></a></div><p>When getting started with local LLMs, one of the very first decisions you have to make is what kind of inference engine [1]  to use. There are quite a few out there, so which one is best? I'll take a look at the three main options: LM Studio, Ollama and llama.cpp.</p>
<p>[1] Inference engine: a program that gives life to the huge data files on HuggingFace.</p>
<h2 id="lm-studio">LM Studio</h2>
<p><a href="https://lmstudio.ai/">LM Studio</a> is primarily a GUI application. It has a user-friendly interface for downloading the models and getting them running. It includes a chat interface where you can quickly converse with the loaded models.</p>
<p>The models that LM Studio uses come from HuggingFace. In HuggingFace there's <a href="https://huggingface.co/lmstudio-community">LM Studio Community</a> which provides a set of curated versions for LM Studio.</p>
<p>Behind the scenes, it uses llama.cpp to do the actual model inference - CPU, CUDA, ROCm, Vulkan &amp; Metal are all supported. In addition to all the llama.cpp-based engines, there's also an Apple's MLX-based engine.</p>
<p>LM Studio provides its own APIs in addition to an OpenAI-compatible API for interacting with the models programmatically, or from agents and IDEs that support OpenAI-compatible providers.</p>
<p>It also has <code>lms</code> CLI utility for people who prefer to interact with it via the terminal.</p>
<h2 id="ollama">Ollama</h2>
<p><a href="https://ollama.com/">Ollama</a> was trying to be the Docker of LLMs (before Docker started supporting running models itself). It is a CLI based application that gives users a very convenient way to download and run models. It focused on developers first, and successfully got itself integrated in various applications that end up being the more user-friendly frontends for it.</p>
<p>The main innovation that Ollama introduced, in my opinion, is automatic model unloading based on VRAM usage. Ollama can keep as many models loaded as memory permits and will unload some of them only when needed to make room for the newly requested model.</p>
<p>There are a few other ideas that they've adopted which differ from the status-quo. For example, Ollama introduced a Docker-like registry for models where files are stored and downloaded based on their SHA-256 hashes. HuggingFace has only recently implemented their "registry" support, so now it's possible to pull GGUFs directly from HuggingFace.</p>
<p>Then there is the Modelfile concept, which is meant to be a Dockerfile-inspired solution to defining models. To me, this seems like an unnecessary - or even wrong - abstraction. If you want to tweak parameters (e.g., <code>num_ctx</code>, <code>num_gpu</code>, <code>use_mmap</code>) to optimise a model for your hardware, you must create a new version of each model you use.</p>
<p>What puzzles me the most is Ollama's decision to replace the the de-facto standard Jinja templates (for chat messages) with Go templates. This means that if one wants to run a newer model on Ollama, you also have to port the Jinja template to Go template syntax.</p>
<p>Choosing the quantization level for a model is also somewhat involved. Ollama defaults to Q4_K_M quantization level for all its published models. If you want to change that, you must first obtain the Modelfile of ollama's model (for the chat template and parameters), then find a GGUF with the desired quantization on HuggingFace and finally combine the two.</p>
<p>Ollama has been using llama.cpp behind the scenes as its inference engine, but it is slowly moving towards its own version. In the short term, I think, it won't be great for users as the new engine has to play catch-up with llama.cpp; in the long term, however, it could be interesting.</p>
<p>Although Ollama relies on llama.cpp, it lags behind in some areas. For example, Vulkan support is still experimental, and Ollama does not support partitioned GGUFs, which are becoming the norm on HuggingFace.</p>
<h2 id="llama-cpp">Llama.cpp</h2>
<p><a href="https://github.com/ggml-org/llama.cpp">Llama.cpp</a> is a C++ library that powers the two offerings above. In addition to being a library that others can build on, llama.cpp ships with <a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#readme">llama-server</a> - a tool that lets you run models via OpenAI-compatible APIs, just like Ollama and LM Studio. There is also <a href="https://github.com/ggml-org/LlamaBarn">LlamaBarn</a>, a light-weight GUI for macOS.</p>
<p>Llama.cpp is very actively developed, support for new models and optimizations for existing ones are added almost daily. The project does not have scheduled releases - what gets implemented (merged) is shipped automatically. This means that you always get the latest changes, but you may also encounter bugs and/or regressions.</p>
<p>Llama-server exposes a lot of knobs to tweak and supports various backends (the main ones being CUDA, ROCm, Vulkan, and CPU). This allows you to experiment with a lot of them to find the best configuration for your hardware. Most of these knobs are also exposed in Ollama &amp; LM Studio, but not all.</p>
<p>The easiest way to run llama-server is via their official Docker images. It is also available through winget, homebrew or nix package managers. Pre-built binaries can be downloaded from the Github releases.</p>
<p>In general, llama-server is best if you want to run the latest models &amp; get the most out of your hardware.</p>
<h3 id="side-note-reusing-models">Side note: Reusing models</h3>
<p>It is perfectly reasonable to use more than one of the tools above. One question that may arise is whether you can download the model once and run in different tools, instead of downloading the data multiple times.</p>
<p>Between LM Studio and llama-server reusing the downloaded models is relatively simple. Some LM studio specific models may use Jinja template functions that don't exist in llama.cpp, but in most cases the models are interchangeable.</p>
<p>Ollama makes this more difficult. Even though it uses GGUFs behind the scenes, it stores everything by hash, so an extra step is required to translate to/from the hash when trying to reuse data.</p>

</article><hr />
	<nav id="post-nav"><a class="post-nav-item post-nav-prev" href="https:&#x2F;&#x2F;www.roosmaa.net&#x2F;blog&#x2F;2025&#x2F;external-dns-technitium-webhook&#x2F;">
				<div class="nav-arrow">Previous</div>
				<span class="post-title">ExternalDNS webhook for Technitium DNS</span>
			</a></nav>
		
	<span id="copy-code-text" class="hidden">Copy Code</span>

	</main>
	<footer id="site-footer">
			<p>&copy; Mart Roosmaa, 2026</p>
		<p>
			<small>Powered by <a class="link external" href="https://www.getzola.org" rel="">Zola</a> and <a class="link external" href="https://duckquill.daudix.one" rel="">Duckquill</a>
			</small>
		</p>
		<ul id="socials">
				<li>
					<a href="https://linkedin.com/in/roosmaa" rel=" me" title="LinkedIn">
						<i class="icon" style='--icon: url("data:image/svg+xml,%3Csvg role=&#x27;img&#x27; viewBox=&#x27;0 0 24 24&#x27; xmlns=&#x27;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&#x27;%3E%3Ctitle%3ELinkedIn%3C&#x2F;title%3E%3Cpath d=&#x27;M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z&#x27;&#x2F;%3E%3C&#x2F;svg%3E")'></i>
						<span>LinkedIn</span>
					</a>
				</li>
				<li>
					<a href="https://github.com/roosmaa" rel=" me" title="GitHub">
						<i class="icon" style='--icon: url("data:image/svg+xml,%3Csvg role=&#x27;img&#x27; viewBox=&#x27;0 0 24 24&#x27; xmlns=&#x27;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&#x27;%3E%3Ctitle%3EGitHub%3C&#x2F;title%3E%3Cpath d=&#x27;M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12&#x27;&#x2F;%3E%3C&#x2F;svg%3E")'></i>
						<span>GitHub</span>
					</a>
				</li>
				<li>
					<a href="https://twitter.com/roosmaa" rel=" me" title="Twitter">
						<i class="icon" style='--icon: url("data:image/svg+xml,%3Csvg role=&#x27;img&#x27; viewBox=&#x27;0 0 24 24&#x27; xmlns=&#x27;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&#x27;%3E%3Ctitle%3ETwitter%3C&#x2F;title%3E%3Cpath d=&#x27;M21.543 7.104c.015.211.015.423.015.636 0 6.507-4.954 14.01-14.01 14.01v-.003A13.94 13.94 0 0 1 0 19.539a9.88 9.88 0 0 0 7.287-2.041 4.93 4.93 0 0 1-4.6-3.42 4.916 4.916 0 0 0 2.223-.084A4.926 4.926 0 0 1 .96 9.167v-.062a4.887 4.887 0 0 0 2.235.616A4.928 4.928 0 0 1 1.67 3.148 13.98 13.98 0 0 0 11.82 8.292a4.929 4.929 0 0 1 8.39-4.49 9.868 9.868 0 0 0 3.128-1.196 4.941 4.941 0 0 1-2.165 2.724A9.828 9.828 0 0 0 24 4.555a10.019 10.019 0 0 1-2.457 2.549z&#x27;&#x2F;%3E%3C&#x2F;svg%3E")'></i>
						<span>Twitter</span>
					</a>
				</li>
				<li>
					<a href="https://bsky.app/profile/roosmaa.net" rel=" me" title="Bluesky">
						<i class="icon" style='--icon: url("data:image/svg+xml,%3Csvg role=&#x27;img&#x27; viewBox=&#x27;0 0 24 24&#x27; xmlns=&#x27;http:&#x2F;&#x2F;www.w3.org&#x2F;2000&#x2F;svg&#x27;%3E%3Ctitle%3EBluesky%3C&#x2F;title%3E%3Cpath d=&#x27;M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479.815 2.736 3.713 3.66 6.383 3.364.136-.02.275-.039.415-.056-.138.022-.276.04-.415.056-3.912.58-7.387 2.005-2.83 7.078 5.013 5.19 6.87-1.113 7.823-4.308.953 3.195 2.05 9.271 7.733 4.308 4.267-4.308 1.172-6.498-2.74-7.078a8.741 8.741 0 0 1-.415-.056c.14.017.279.036.415.056 2.67.297 5.568-.628 6.383-3.364.246-.828.624-5.79.624-6.478 0-.69-.139-1.861-.902-2.206-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8Z&#x27;&#x2F;%3E%3C&#x2F;svg%3E")'></i>
						<span>Bluesky</span>
					</a>
				</li>
		</ul>
</footer>


</body>
</html>
